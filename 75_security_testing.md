# Security Testing

When computers were first starting to become a thing (that's a technical term), security was not a key driver behind developing software for them.  Systems were rarely networked, so in order to tamper with the system or view its data, you would have to physically get to the computer.  Even in the 1950s, people knew how to secure physical locations (locks and security guards are pretty useful in the real world). In the rare instances that computers were networked or publicly accessible, there were few users, and they tended to be grad students, developers, or other authorized users.  Supposing somebody was able to break into a system, it was usually not a big deal.  Richard Stallman, the founder of the Free Software Foundation, argued against having passwords in the operating system ITS. If you wanted to crash a system running ITS, all any user had to do was type `KILL SYSTEM`.  In other words, people generally trusted each other to do the right thing.  If you're interested in reading about this Golden Age of Computing, I recommend _Hackers: Heroes of the Computer Revolution_ by Steven Levy.

The title of that book is a good place for a quick tangent.  The term "hacker", although often used to mean "one who maliciously breaks into computer systems", is actually of a much older vintage.  It meant one who did something clever, new, or interesting.  Despite the world's insistence on the term "hacker", I will be using the term "cracker" to mean a person who breaks into systems which they are not authorized to access.

Anyways, humanity being what it is, the idyllic state of affairs where computer users mostly trusted each other was not meant to last.  Although there were few networked computers in the 1960s, there was a huge, networked system that reached a huge percentage of the population---the phone system.  People known as "phone phreaks" would explore the phone system, figuring out how to make devices that would allow for free incoming calls or fool pay phones into thinking that coins were dropped into them.  While the primary goal of most of these phone phreaks was simply understanding the labyrinthine complexity of the phone system, telephone security personnel and police saw it is as an early form of "digital trespassing".

As the world became more and more networked, stories of software security failures became commonplace.  Among these were movies such as _WarGames_, which included an artificial intelligence in charge of the United States' nuclear arsenal being fooled by a teenage computer whiz, and books such as _The Cuckoo's Egg_, which detailed the true story of a former astronomer tracking down a West German computer cracker.  Real-life events, such as the Morris Worm, which shut down a large portion of the Internet in 1988, also helped to mark the introduction of security as a risk that programmers needed to concern themselves with.  The concept of __information security__ was born.  Software started to include features which would prevent unauthorized access or modification of data.

Today, breaking computer security is a big business, with many crackers infiltrating systems in order to blackmail companies, steal credit card information, or shut down websites which they find objectionable.  Likewise, protecting systems from unauthorized access, and helping to find weak points in a system before the "bad guys" do, is also a big business.

## Challenges in Security Testing

Security testing differs from other kinds of software testing in that there is an intelligent adversary---indeed, many adversaries, although not all of them are really "intelligent"---also looking for defects.  You can think of malicious crackers trying to break into your system as just a variant of a tester---they are constantly testing for weak points in your software that they can use to gain access or steal data.  When you're unit testing, you don't need to worry that the system will change from under you or _try_ to cause your tests to fail, but this is definitely the case in security testing.

In some ways, you have it much harder than your adversaries.  The common---some would say clich√©---metaphor is that of defending a castle.  If the enemy finds one unlocked and undefended door, they will have the run of the castle.  You, on the other hand, need to ensure that you have defended every single entrance.  Similarly, you will have to check for all of the weak points in your software, while they only need to find one.  It doesn't matter if you are protected from a SQL injection attack if the attacker is easily able to find passwords from an insecure storage vulnerability.

Although there's still a _mythos_ involving computer security where those trying to break into computers are just curious kids (although this is still the case sometimes!), many attacks are done for some sort of financial remuneration.  People sell botnet time to the highest bidder to send spam.  They develop and sell exploits to "offensive hacking" groups or unnamed government agencies.  They break into a retailer's credit card database not to see if it can be done, but because they want to buy things with credit cards (or sell them on the black market and get money that way).  There is a huge market for security information and for "secret" data.

Security testing requires you to think like your adversary.  In order to test a system effectively, you will need to stop thinking like the upstanding person I'm sure you are. Instead, you will need to think like someone who will do sneaky things to get into the system and approach it in a way that ordinary users would not.  After all, if there were vulnerabilities which could be found by routine use of the software, they have probably already been discovered and fixed.  The most dangerous defects tend to be the ones that use the system in ways that normal people generally don't even conceive.  Again, this is very different from standard functional testing, where the most dangerous defects are the ones that ordinary users will hit into the most!  If your text editor fails whenever you hit the `E` key, for example, that's a major defect with huge impact, basically making the text editor useless.  This assumes, of course, that you are not Georges Perec writing _La Disparition_---an obscure corner case.  If it only fails whenever the user types "`%&#@@!_!<Ctrl-R><Ctrl-Q>`", well, that defect may not even be worth fixing.  The only people who will try it are the ones reading this book and wondering if it's really a defect in some text editor (I'll never tell).  However, if the same defects allowed access to a bank account, the reverse might be true.  The fact that `E` causes the failure means that it will be caught readily; perhaps only a dedicated attacker will figure out to type "`%&#@@!_!<Ctrl-R><Ctrl-Q>`" and it flies under the radar for months as more accounts are mysteriously depleted.

Security failures can be catastrophic and tend to have worse impacts than similar functional failures.  Assuming a good backup strategy and other simple safety procedures, most functional or non-functional defects will not cause much havoc.  Not all defects are trivial, of course; there are numerous examples of software defects causing problems up to and including loss of life (for a famous case, see _An Investigation of the Therac-25 Accidents_ by  N. Leveson and C.S. Turner, which details how a software race condition caused massive overdoses of radiation for several patients).  However, the vast majority of defects do not have an impact quite so dire.  The impact of security-related defects, however, can be multiplied by the attacker.  If there is a race condition that makes brakes temporarily inoperable on a car, this is bad.  However, if the vulnerability allowing this can be exploited at will by an attacker, it can be timed to make it much, much worse---after all, if your brakes fail when parked or when going very slowly, then the only damage may be some adrenaline and a scuffed fender.  If they are disabled deliberately and maliciously, the attacker may wait until you're going down a steep grade, where the damage is likely to be much more serious.  If your software accidentally allows a bank account to be credited an extra thousand dollars under certain, but nondeterministic, circumstances, it may not be a big issue at all---eventually the bank notices the extra money in the account and removes it.  If somebody can deliberately trigger it, they're going to go to the ATM as soon as possible and start dispensing cash, which is a much more difficult problem to fix.

Finally, just because you have found every single vulnerability in your system's code and fixed them until the system is entirely bulletproof, all of your hard work can be undone by somebody calling one of your users, saying that they're the Allegheny County Password Police, and would they mind confirming their password.  This happens more often than you might think---read _Ghost in the Wires_ by Kevin Mitnick for some great stories about successfully using similar techniques.  People are often confounded by technology and information security, or simply aren't paying that much attention to what somebody's asking them.  As Georgia Weidman, author of _Penetration Testing_, says, "Users are a vulnerability that can never be patched."

## Basic Concepts in Computer Security

Although we've already used some of them, it's time to make some formal definitions of security-related terms so that we can discuss them in more detail.

The key element of security is the __InfoSec Triad__, also called the __CIA Triad__.  This consists of the following three attributes of a system: __confidentiality__, __integrity__, and __availability__.  Confidentiality means that no unauthorized users may read any data.  This could be data on the entire system (for example, the root user on a Unix system has access to everything stored on it), or a specific piece of data (an engineer may be authorized to view the source code for a system, but not employee payroll information; a financial auditor may be able to view the latter but not the former).  Integrity means that no unauthorized users can write data; for example, anybody is free to look at the source code for Rails (a Ruby web framework), but only a select few are allowed to directly write to it (although you may submit a pull request asking those people to merge your changes).  Given these definitions, it would be extremely simple to create a secure system---after creating the system, smash it with a sledgehammer, encase it in concrete, and drop it to the bottom of the Marianas Trench.  No unauthorized users will be reading or writing any data from it now!

Of course, no authorized users would be able to use it, either, and few customers would be willing to pay for a system that nobody can access.  In order to prevent such a trivial but non-useful solution for creating secure systems, the final element of the InfoSec Triad is necessary: availability.  That is, the system must be available to authorized users to read and write data.  A system which has all the elements of the InfoSec Triad under all circumstances is considered secure.  In practice, for most software there will often be particular circumstances where one or more of the elements is not met.

Attacks on the security of a system can be either __active attacks__ or __passive attacks__.  Active attacks actually change the system under attack somehow, such as by adding an additional program that runs in the background, changing users' passwords, or modifying data stored on the system.  Passive attacks do not cause any changes to the system.  Examples of passive attacks would be eavesdropping on network traffic or monitoring an area for unsecured wireless networks.  While active attacks often cause more damage, they are also much easier to spot, whereas passive attacks can be very difficult to observe.

There are several kinds of attacks, both passive and active, possible on elements of the InfoSec Triad.  An __interruption__ attack is an attack on availability.  It reduces or eliminates the availability of a given system.  Perhaps the simplest version of this attack would be somebody sneaking into a building and unplugging all servers from the network.  More advanced attacks would include sending so many unauthorized requests that authorized ones cannot get through (referred to as a __denial of service__ attack) or changing all users' passwords.

An __interception__ attack is an attack on confidentiality.  It enables an unauthorized user to read data which they are not authorized to read, even if they are not able to actually change any data.  Just because they cannot directly change any data does not mean this is a lesser form of attack.  Think of how much harm could befall you if an attacker gained access to your credit card number, social security number, and other identifying information.  The simplest version of an interception attack would be peering over somebody's shoulder when they are typing in their password.  There are much more technically challenging ones such as __keylogging__ programs and hardware, which store and/or transmit any keypresses you make, or __packet sniffing__, where a packet analyzer inspects any packets going over your network looking for passwords or other interesting data.

There are two related kinds of attacks on integrity: __modification__ attacks which modify already-existing data, and __fabrication__ attacks which add additional data to the system.  A modification attack might change the current gift card balance in an account on an e-commerce site, whereas a fabrication attack might add an entirely fictitious user to an account.

If a system has a way that one of the attacks can be utilized against it, that system has a __vulnerability__.  For example, let's assume that a system is set up with a default administrative user, DEFAULT, with password DEFAULT.  If nobody ever discovers this vulnerability (although this is unlikely for a system with even a nominal number of users), then there's never any actual damage done to the system.  However, if a user discovers and uses it, this is an __exploit__ of the vulnerability.  An exploit is a technique or mechanism which is used to compromise one of the elements of the InfoSec Triad of a system.  This can range from knowing that there is a default password to complex pieces of software which interact with the system in known ways to cause the undesired behavior.

There is a wide variety of these tools, which are known as __malware__.  Malware is any software which is deliberately designed to have an undesired effect on a computer system, generally unbeknownst to the authorized user of the system.  An incomplete list of the kinds of malware includes:

1. __Bacteria__: A program which consumes an excess amount of system resources, perhaps taking up all file descriptors or disk space.
2. __Fork bomb__: A special kind of __bacteria__ which continually forks itself, causing all CPU resources to be used up creating more copies of the fork bomb.
2. __Logic bomb__: Code within a program which executes an unauthorized function, such as deleting all data on the first day of the month.
4. __Trapdoor__: A program or piece of a program which provides secret access to a system or application.
4. __Trojan Horse__: A piece of software which pretends to be another in order to trick users into installing and executing it.  For example, a Trojan Horse may state that it contains different funny mouse cursors, but after installing it, it deletes everything on your hard drive.
5. __Virus__: A computer program, often small, that replicates itself with human intervention.  This intervention could be something such as clicking on a link or running a program sent to you as an attachment.
5. __Worm__: A computer program, often small, that replicates itself without human intervention.  For example, once installed on a machine, it may have that machine try to break into other machines and copy the code of the worm over to others.
6. __Zombie__: A computer with software installed which allows unauthorized users access to it to perform unauthorized functionality.  For example, a system might have a mailer program built in which will allow other users to send spam from your machine, so that the actual senders cannot be tracked.
6. __Bot network__: A collection of zombies controlled by a master.
7. __Spyware__: Software which surreptitiously monitors the actions of the user of the system.  For example, software may report back daily what all of the keystrokes of the user were.
8. __DoS Tools__: Tools which enable denial of service attacks.
9. __Ransomware__: Software which performs an unwanted action (e.g., encrypting your hard drive) and asks for money or other compensation in order to undo it.  This money usually goes to the creators or users of the software, not the software itself (until artificial intelligences become more advanced).

Malware may belong to more than one variety.  For example, a program could propagate like a computer worm, and once resident on a computer, report back the activities of the user, thus making it spyware.  Another possibility is that a program is offered for download which states that it will "clean your registry" but will actually make your computer a zombie and under the control of a bot network.

It is not necessary for malware to be involved for there to be an attack on a system.  Just as there are automated tests and manual tests, there are automated attacks and manual attacks.  If I can guess the password of a user or send in a string to a text box which causes the system to crash, I am attacking the system without any kind of program.  While nowadays many of these vulnerabilities are found and exploited with software tools, good ol'-fashioned manual exploitation is still used on a regular basis.

## Common Attacks and How to Test Against Them

### Injection

In an __injection attack__, the attacker is attempting to get your computer to run some arbitrary code of their choosing.  One of the most common types of injection attacks is a __SQL injection attack__, since many programmers, especially new ones, do not __sanitize__ their database inputs.  Sanitization involves ensuring that input from a user will not be directly executed, by "cleaning it up" so that it can't run.  As an example, imagine some code that accepts a string from the user asking for their name, searches in the database for any users with that name, and returns the unique ID (uid) for the first user with that name:

```java
public int findUidByName(String name) {

    Result dbResult = DatabaseConnection.executeSQL(
      "SELECT uid FROM users WHERE name = '" + name + "';");
    if (dbResult == null) {
        return NO_RESULTS_CODE;
    } else {
        return dbResult.get("uid");
    }
}
```

This is a relatively simple method, but contains a glaring security flaw---there is nothing preventing the user from sending in other SQL commands instead of a name.  These SQL commands will be executed by the machine uncomplainingly, even if the commands say to delete the database.  Imagine that the user passes in the value "`a'; DROP TABLE Users;`" as their name.  The following SQL will be executed:

```sql
SELECT uid FROM users WHERE name = 'a'; DROP TABLE Users;
```

You can try this out on your local database if you don't believe me, but that will delete the `users` table when it's executed.  This is probably not something you want anyone who can search for a user in your application to be able to do!  There are numerous other kinds of injection attacks, for example by mis-using `eval()` in JavaScript code, or adding "`OR 1=1`" to a SQL query to show all rows instead of the one that the code would ordinarily be looking for.  The common denominator in injection attacks is that they all execute code that the designers and administrators of the system do not want to have executed.

As mentioned, this can be avoided by sanitizing input in various ways.  For example, semicolons, tick marks, and spaces may not be allowed in a user name.  A check could be added so that if any of those characters exist in the name parameter, then the `NO_RESULTS_CODE` would be returned before any SQL could be executed.  This technique is a simple blacklist, and there are more in-depth ways of preventing injection attacks, but they are beyond the scope of this book.

How does one test that injection attacks are not possible?  The most straightforward way is to find all places where user input is accepted and ensure that sending in code will not result in it being executed.  This will often be a form of grey-box testing, as it wouldn't be very helpful to check that, say, COBOL code could be executed on a Java-only system.  White-box testing, including unit testing, will also be helpful to ensure that methods that receive user input ensure that it is sanitized before passing it on, or that methods which access the database can never be called with code that can exploit a vulnerability.  Static analysis tools also exist which can check the codebase statically to help guard against possible injection attacks.

Stochastic testing---especially "evil monkey" testing, which passes in executable code---can be helpful for large systems which request and process input in many different ways and in different places.  By passing in large amounts of random data, you may find kinds of data which cause the system to perform oddly or crash.  These out-of-the-ordinary events may help point you to which specific parts of the system are vulnerable to injection attacks.  For example, if a particular parameter is parsed with an `eval()` at some point, your randomized testing may pass in some invalid code, causing the system under test to crash.  Closer examination of the code which handles that kind of data will allow you to determine that code injection is possible there.

### Buffer Overruns

What happens when you try to put ten pounds of data in a five-pound bag?  A __buffer overrun__, that's what.  In many programming languages, you have to allocate a finite amount of space for data to be put into.  In Java, for example, if you wanted to store five integers in an array, you could do something like this:

```java
int[] _fiveInts = new int[5];
```

Now let's say that you have a method that accepts a string of integers, separated by commas (e.g. `7,4,29,3,2`), and then puts each of the integers into the `_fiveInts` array:

```java
public void putDataIntoFiveInts(String data) {
    int[] intData = data.split(",");
    for (int j=0; j < intData.length; j++) {
        _fiveInts[j] = intData[j];
    }
}
```

This works fine if `7,4,29,3,2` is passed in to the method.  However, if `1,2,3,4,5,6,7` is passed in, then an `ArrayIndexOutOfBounds` exception will be thrown when trying to write the sixth piece of data to `_fiveInts`.  This may cause the program to crash if the exception is not properly handled.  In some languages, such as C, no exception is thrown, because there is no __bounds checking__ (checking at run-time that data is not being written outside the bounds of an array).  The system will keep on merrily writing data past the end of the array, which may overwrite executable code or other system data.  If this data is carefully crafted, it may even allow the attacker to get shell access to the system.

This can be tested for by passing in very large amounts of data to all places where input can be expected into the system.  The amount of data to be passed in will vary, and can often be determined by checking the code (thus making grey- or white-box testing very effective when checking for this particular kind of vulnerability).  Static analysis tools can also help determine where buffer overruns are possible.  Finally, using a language that has built-in bounds checking, like Java, can help mitigate the problem.  It is usually better for a program to stop running due to a missed exception bubbling up than code or data being overwritten.

### Security Misconfiguration

Although your system may operate in a bulletproof way if it's set up correctly, not everybody is going to be as rigid as you are when setting up their version of the system.  People leave default passwords set, or give everybody read/write access because it's easier that way, or don't turn on two-factor authentication because they hate having a program text them every time they log in.  For complex programs, people may miss the one checkbox that encrypts data, or enables HTTPS, or requires users to log in before modifying data.

In order to avoid these, you want to have a sensible set of defaults for your software, and ensure that users understand the weak points of the application that they have set up.  They should also be able to easily determine how to remedy these weak points that they have created.  If the only way to properly set up the system is to thoroughly read through the 500-page instruction manual and set some obscure command line switches, then virtually every system you ship will be configured improperly.

How do you test for them, though?  Often, you will have to do some form of __user testing__.  User testing involves having a user perform some task, often with minimal---or no---guidance from the development team or their representatives.  While this is usually done in order to determine the best user interface for a system, it can also be done to figure out how typical users configure the system and what parts of the system they do not configure properly.  After seeing that users often forget to change default passwords, for example, perhaps developers add a red warning to all administrative pages warning them that the default password has not yet been changed.  Further user testing can verify that this causes the desired change in behavior.

### Insecure Storage

Even if the code running your system is secure itself---free from all known exploits, all input sanitized, formally verified to not contain any buffer overflows---this is of little consolation if its data is not stored properly!  Examples of insecure storage would be writing sensitive data to log files, allowing users direct access to a database, or storing private keys in your code which is stored in a publicly available repository.

Note that this can be more tricky to verify than simply checking the log files or searching for passwords hard-coded into your program.  For example, under normal circumstances, only boring debug data may be sent to a log file, and so you think that, even though the log file is publicly accessible, it is not a large security risk.  However, if an error occurs when processing a credit card, an exception is thrown which includes debug data.  In this debug data is the credit card information that the system was attempting to process.  Under these circumstances, if this exception is written to the log file, the fact that the log file is publicly accessible is a very big problem.

Testing for insecure storage can be as straightforward as attempting to access data directly on the database or on the filesystem.  However, it can get more complex in larger systems.  In general, you want to follow the __principle of least privilege__, which means limiting users to the minimal amount of access that they need to do their job.  Developers do not require access to personnel records; likewise, the head of Human Resources does not need to have access to the source code for a system.  Checking for insecure storage can also involve automated checks, which may run before code check-in, that verify that no private keys, passwords, or similar secrets are checked into the repository.

### Social Engineering

This is it, the king of attacks, the most common way of exploiting systems everywhere---going through "the vulnerability that can never be patched", people.  __Social engineering__ involves manipulating people (often authorized users of a system) to underhandedly cause them to perform actions that put the security of a system at risk.  Some examples would be telling a user of a system that they are from the IT department and need to know their password for "routine maintenance", or an email that has a forged "from" field asking the user to run the Unix command `chmod -R 777 *` in their home directory so that testing can commence.

While these may seem ridiculous to many readers of this text (and if they are not, then remind me to put you in touch with a certain deposed nobleman looking to give away some of his millions), social engineering is used to access many systems.  One common method is __phishing__---trying to get personal or other sensitive information via email or other communications.  These are usually sent to a wide variety of email addresses, hoping that somebody responds and will fall for their shenanigans.

Phishing attacks often seem very poorly prepared, with broken English and technical inaccuracies, but this is actually a part of the plan!  There are usually multiple steps after the initial contact phase in order to achieve the primary goal of the phishing expedition.  Let's say that the target received an email stating that their email account was compromised and they have to click [here](http://www.example.com "Fake Company") to verify their password (if you're not reading this online, links don't work as well on dead-tree books, but don't worry, you're not missing much).  If the user clicks the malicious link, they will be taken to a page created by the attackers (and which may not look exactly like the actual, legitimate account information page) that allows them to steal the user's password.  Ideally, the user won't check with their IT department afterwards about the email; the attackers want time to use the email for whatever malicious people do with stolen email accounts.  In other words, they want people who are not very conscientious or technically literate, who overlook minor issues, who are trusting of whatever they see in front of them.  These are the same people who would overlook the poor grammar and inaccuracy of the original email.  People who are less trusting might be even more work for the attackers, as they may deliberately enter false data or even work on trying to track them down.  The poorly constructed email is actually a screening mechanism.

A much more dangerous variant of phishing is __spear phishing__, in which the user is specifically targeted.  In this case, the attacker goes out of their way to ensure that the user will not be suspicious of the email.  Relevant details will be carefully crafted: any other users mentioned in the email will be verified, grammar will be excellent (or at least appropriate), the user's actual name will be used, its headers will be forged to look like it came from the targeted user's boss, etc.  Spear phishing attacks are much more difficult and time-consuming to set up than a traditional phishing attack, but they also tend to be more effective; think of them as precision-guided munitions compared to the carpet bombing of a regular phishing attack.  Even experienced users may have difficulty determining that the email is not legitimate.

Testing that social engineering will not work on a system is difficult.  After all, there is no technical aspect to check; the programmed aspects of the system would work correctly, and by authorized users, but authorized users who are actually doing the bidding of an unauthorized user.  While we have discussed the difficulty of testing impure functions, which may rely on global variables or other external mutable state, people are the ultimate unpredictable external state.  A person who may never fall for a phishing email may be tricked by somebody calling them; someone who would normally never fall for anything may have had a rough night the evening before and isn't thinking clearly; someone else may be overwhelmed with other work and click on a malicious link in an email.

Ensuring that user accounts follow the principle of least privilege will limit the extent of any damage that can be done by an attacker acting through the authorized user.  For example, an ordinary user of our Rent-A-Cat system should never have access to the payroll system for employees.  This will help to compartmentalize the attack to only the data and functionality to which the social engineering victim has access.  Checking if there is any way for a user to access data (read or write access) that they don't need to will help to find possible areas where this access can be compromised.  

Running tests on people can be helpful, as well!  Some companies send fake phishing emails and see how many of their people click on suspicious links.  Those who do click on the links are sent to a page warning them that this could have caused unauthorized access to the system, and those employees will (hopefully) be less likely to do so when they receive an actual phishing email.

## Penetration Testing

While I've gone over various ways in which the security of a system can be compromised, often the best way to test a system is to think like an attacker.  __Penetration testing__ has a user, often external to the development team, attempt to gain unauthorized access to a system using any means at their disposal (tempered by the limits of the contract, e.g. "no deleting data").  This follows the "set a thief to catch a thief" theory---those who are acting like the people who are trying to break into your system will be the most effective at finding any holes in your security systems.

During a penetration test, a person will act as though they were a cracker trying to get in to your system.  They may gather data to determine which operating systems or programming languages you use and scan your networks, try common passwords, or use other tactics and techniques specific to the system that you're running.  These are often external consultants or at least personnel unrelated to the team developing it, so as to avoid preset theories of how the system is "supposed" to work.  The entire _modus operandi_ of people trying to break into systems is that they manipulate systems to do things that they are not "supposed" to do.

A penetration tester will then develop a report of what the weak points of the system are, as well as the ramifications of those weak points.  For example, they may find a SQL injection vulnerability for one particular subsystem which allows access to a particular database.  While this may sound like technical gobbledygook to a manager, explaining the ramifications---that all of the payroll data for the company is available to anybody with an Internet connection---is much clearer (and scarier).

## General Guidelines

Before developing a testing plan for security, you should try to determine how much testing will be necessary.  This will depend on the domain in which you are operating, but mostly on what the risks are if an opponent is able to compromise the system.  Remember that time and resources spent on security testing are time and resources that are not spent elsewhere.  If you are running a system which controls nuclear weapons launch codes, it makes sense to spend a large percentage of the time testing the system on security testing; if you are running a startup for renting cats, less time and fewer resources are probably necessary.

If you are operating in a regulated field, ensure that you are following all of the standards for that field.  For example, in the United States, if you are storing medical data, you should be familiar with HIPAA; if you are dealing with student data, you should read up on FERPA.  Although these are not the be-all, end-all of security for a system, they are additional parameters you should know about while determining its security.

Determine what are the most important aspects of the system to guard against.  Attacks which may cause data to be overwritten will in most cases be more damaging than those that allow read access to data.  Certain aspects of the system may be more damaging if they are compromised.  If there is an employee-run wiki which is mostly full of in-jokes and Dungeons and Dragons schedules, this is less important than bank account information.  This doesn't mean that the employee wiki should be ignored, but there should definitely be more emphasis put on the bank account data.

Remember the Pareto Principle, as well.  By performing a broad check, spending not much time on each individual component, you may be able to find many more defects, and more easily-found defects, than spending months and months on one particular part.  These will often also be the first vulnerabilities looked for by potential attackers.  Even if a very difficult-to-exploit vulnerability exists on a given system, most attackers will be deterred if many of the common attacks are ineffective.

The most effective security testing is that which gets done.  If you can, argue for the security tests that you not only believe will be best for the system, but also have a chance to get done and to continue to get done.  In today's networked world, security is becoming more and more of an ongoing practice, as anybody who has gotten a pop-up asking them to upgrade their software is aware.  You should develop a plan which will allow ongoing security maintenance and verification as long as the system is potentially exploitable.
